import os
import requests
import pdfplumber
import docx
import argparse
from dotenv import load_dotenv

# ---------------------------
# Load environment variables (safely retrieve API keys and endpoint URLs from external file)
# ---------------------------
load_dotenv("config.env.template")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_URL = os.getenv("PINECONE_URL")  # Must not include /query at the end


# ---------------------------
# Text extraction process according to file type
# PDF → pdfplumber
# Word → python-docx
# Others → Read directly in UTF-8 (e.g., .txt, .md, .py)
# ---------------------------
def extract_text(file_path):
    ext = os.path.splitext(file_path)[1].lower()
    try:
        if ext == ".pdf":
            text = ""
            with pdfplumber.open(file_path) as pdf:
                for page in pdf.pages:
                    text += page.extract_text() + "\n"
            return text
        elif ext == ".docx":
            doc = docx.Document(file_path)
            return "\n".join([para.text for para in doc.paragraphs])
        else:
            with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                return f.read()
    except Exception as e:
        print(f"[Warning] Text extraction failed: {file_path} → {e}")
        return ""


# ---------------------------
# Chunking process (split long text into specified byte size)
# chunk_size: Number of characters per chunk (e.g., 1000 chars)
# overlap: Overlap between chunks (to preserve context)
# → Important preprocessing step to improve RAG accuracy
# ---------------------------
def chunk_text(text, chunk_size=1000, overlap=200):
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start += chunk_size - overlap
    return [c.strip() for c in chunks if c.strip()]


# ---------------------------
# Generate embeddings via OpenAI API
# Model: text-embedding-3-small (high-accuracy version from 2024 onward)
# Vectorize each chunk for semantic search with Pinecone
# ---------------------------
def get_embedding(text):
    headers = {
        "Authorization": f"Bearer {OPENAI_API_KEY}",
        "Content-Type": "application/json",
    }
    data = {"input": text, "model": "text-embedding-3-small"}
    response = requests.post(
        "https://api.openai.com/v1/embeddings", headers=headers, json=data
    )
    response.raise_for_status()
    return response.json()["data"][0]["embedding"]


# ---------------------------
# Upload process to Pinecone
# ID: Uniquely generated by filename + chunk number
# namespace: Logical group specified by user (switchable by purpose)
# metadata: Stores original text and file info to return during search
# ---------------------------
def upload_to_pinecone(vector_id, embedding, metadata, namespace):
    headers = {"Api-Key": PINECONE_API_KEY, "Content-Type": "application/json"}
    url = f"{PINECONE_URL}/vectors/upsert"
    data = {
        "vectors": [{"id": vector_id, "values": embedding, "metadata": metadata}],
        "namespace": namespace,
    }
    response = requests.post(url, headers=headers, json=data)
    if response.status_code != 200:
        print(f"[Error] Upload failed: {vector_id} → {response.text}")
    else:
        print(f"[Success] Uploaded: {vector_id}")


# ---------------------------
# Split full text of a single file → vectorize → register in Pinecone
# Upload and log each chunk
# ---------------------------
def process_file(file_path, namespace):
    text = extract_text(file_path)
    if not text.strip():
        print(f"[Skip] Empty or unextractable: {file_path}")
        return
    chunks = chunk_text(text)
    for idx, chunk in enumerate(chunks):
        vector_id = f"{os.path.basename(file_path)}-chunk-{idx+1}"
        try:
            embedding = get_embedding(chunk)
            metadata = {"source": file_path, "text": chunk}
            upload_to_pinecone(vector_id, embedding, metadata, namespace)
        except Exception as e:
            print(f"[Error] Failed while processing {vector_id}: {e}")


# ---------------------------
# Traverse all files in directory and process sequentially
# Recursively includes subdirectories
# ---------------------------
def process_directory(directory_path, namespace):
    for root, _, files in os.walk(directory_path):
        for file in files:
            file_path = os.path.join(root, file)
            print(f"[Processing] {file_path}")
            process_file(file_path, namespace)


# ---------------------------
# Parse command-line arguments and execute
# directory: Target folder containing documents (e.g., Flask/PDF)
# namespace: Logical group name on Pinecone to store vectors
# ---------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("directory", help="Target directory (e.g., Flask/PDF)")
    parser.add_argument(
        "namespace", help="Pinecone namespace (e.g., our-project-specs)"
    )
    args = parser.parse_args()

    # Check folder existence
    if not os.path.exists(args.directory) or not os.path.isdir(args.directory):
        print(f"[Error] Directory does not exist: {args.directory}")
        exit(1)

    # Check if folder is empty
    if not any(os.scandir(args.directory)):
        print(f"[Error] Directory is empty: {args.directory}")
        exit(1)

    # Start batch processing
    process_directory(args.directory, args.namespace)
