from dotenv import dotenv_values
import openai
from pinecone import Pinecone

# ---------------------------
# Load environment variables (directly from config.env.template)
# ※ Does not depend on system environment variables; reads keys/settings from file
# → Improves security and portability; suitable for switching between test/prod environments
# ---------------------------
config = dotenv_values("config.env.template")

# Set OpenAI API key directly (not via environment variable)
openai.api_key = config.get("OPENAI_API_KEY")

# Initialize Pinecone with direct API key
pc = Pinecone(api_key=config.get("PINECONE_API_KEY"))

# Retrieve Pinecone index name from external config
# → Flexible structure for handling multiple projects/datasets
index = pc.Index(config.get("PINECONE_INDEX_NAME"))


# ---------------------------
# Similar document search (Pinecone + OpenAI embedding)
# Input: Question (natural language), namespace (dataset identifier)
# Output: List of matched metadata["text"]
# ---------------------------
def get_similar_chunks(question, ns):
    # Vectorize question using OpenAI API (embedding model)
    embedding = (
        openai.embeddings.create(
            input=question,
            model="text-embedding-3-small",  # Lightweight and high-accuracy embedding model
        )
        .data[0]
        .embedding
    )

    # Execute similarity search in Pinecone vector DB
    results = index.query(
        vector=embedding,
        top_k=5,  # Retrieve top 5 similar documents
        include_metadata=True,  # Include metadata with original text
        namespace=ns,  # Identifier for logical separation by project or use case
    )

    # Extract and return only the text content from metadata
    return [match["metadata"]["text"] for match in results["matches"]]


# ---------------------------
# Response generation (OpenAI Chat model)
# Input: User question, namespace (target dataset)
# Output: Answer string in natural language generated by gpt-4o
# ---------------------------
def ask_direct_answer(question, ns):
    # Retrieve similar documents and use as context for the answer
    chunks = get_similar_chunks(question, ns)
    context = "\n".join(chunks)

    # Prompt design: Instruct to generate answer based on FAQ-style documents
    prompt = (
        f"以下の情報に基づいて質問に答えてください:\n\n{context}\n\nQ: {question}\nA:"
    )

    # Generate response using gpt-4o (latest as of 2024)
    response = openai.chat.completions.create(
        model="gpt-4o", messages=[{"role": "user", "content": prompt}]
    )

    # Extract and return only the generated answer text
    return response.choices[0].message.content.strip()
